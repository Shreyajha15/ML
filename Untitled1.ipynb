{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1OJz3Rqj4g0T6R7Ct4Y-z7k8JLA66oS29",
      "authorship_tag": "ABX9TyNbho8wwDOeBcWtQf0Ikbiv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreyajha15/ML/blob/master/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei0H2Y-FJdWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "from google.colab import drive\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "havKOQ3FO9j7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tensorflow Libraries\n",
        "try:\n",
        "  % tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56jiyiWFPBp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fetyching data from drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UJo_XxiNsyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols=[\"sentiment\",\"id\",\"date\",\"query\",\"user\",\"text\"]\n",
        "train=pd.read_csv(\"/content/drive/My Drive/Data/train.csv\",\n",
        "           \n",
        "          header=None, names=cols,engine=\"python\",encoding=\"latin1\")\n",
        "\n",
        "test=pd.read_csv(\"/content/drive/My Drive/Data/test.csv\",\n",
        "           \n",
        "          header=None, names=cols,engine=\"python\",encoding=\"latin1\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G8ZqzA3W66E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head(3)\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "  tweet=BeautifulSoup(tweet,\"lxml\").get_text()\n",
        "  tweet=re.sub(r\"@a-zA-Z0-9]+\",\" \",tweet)\n",
        "  tweet=re.sub(r\"https?://[a-zA-Z0-9./]+\",\" \",tweet)\n",
        "  tweet=re.sub(r\"[a-zA-Z.!?']\",\" \",tweet)\n",
        "  tweet=re.sub(r\" +\",\" \",tweet)\n",
        "  return tweet\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4b82isKgHwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_clean=[clean_tweet(tweet) for tweet in train.text]\n",
        "train_clean=[clean_tweet(tweet) for tweet in test.text]\n",
        "data_clean=[clean_tweet(tweet) for tweet in data.text]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMNi1fS6gXj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_labels=train.sentiment.values\n",
        "data_labels[data_labels==4]=1\n",
        "set(data_labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGCAFDhIFiOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer=tfds.features.text.SubwordTextEncoder.build_from_corpus(data_clean,target_vocab_size=2**16)\n",
        "data_input=tokenizer.encode(x) for x in data_clean\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOQPUuxbGIY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlength1=max(len(sent) for sent in input])\n",
        "data_input=tf.keras.preprocessing.siquence.pad_sequences(data_input,value=0,padding='post',maxlen=maxlenght1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdBWg6nx-ke6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data=np.randint(0,8000000,80000) #The first half have negative tweets and secod half have postive tweets\n",
        "test_data=test_data.concatenate(test_data+8000000) #Second half is added to the first half randomly accessed numbers\n",
        "test_input=data_input(test_data) #choose input data\n",
        "test_label=data_labels(test_data) #give labes to the test set\n",
        "\n",
        "#Remove this from train set to get the reamining data as train set\n",
        "train_input=np.delete(data_input,test_data,axis=0)\n",
        "train_labels=np.delete(data_labels,test_data)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mIZnqRILQw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DCNN(tf.keras.models):\n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               emb_dim=128,\n",
        "               nb_filters=50,\n",
        "               FNN_units=512,\n",
        "               nb_classes=2,\n",
        "               dropout_rate=0.2,\n",
        "               training=False,\n",
        "               name='DCNN'):\n",
        "    super(DCNN,self).__init__(name=name)\n",
        "    self.embedding=layers.embedding(vocab_size,emb_dim)\n",
        "    self.bigram.Layers.Conv1D(filters=nb_filters,\n",
        "                              kernel_size=2,\n",
        "                              padding=\"valid\",\n",
        "                              activation=\"relu\")\n",
        "    self.pool1=layers.GlobalMaxPool1D()\n",
        "    self.tigram.Layers.Conv1D(filters=nb_filters,\n",
        "                              kernel_size=3,\n",
        "                              padding=\"valid\",\n",
        "                              activation=\"relu\")\n",
        "    self.pool2=layers.GlobalMaxPool1D()\n",
        "    self.fourgram.Layers.Conv1D(filters=nb_filters,\n",
        "                              kernel_size=4,\n",
        "                              padding=\"valid\",\n",
        "                              activation=\"relu\")\n",
        "    self.pool3=layers.GlobalMaxPool1D()\n",
        "\n",
        "\n",
        "    self.dense_1=layers.Dense(units=FFN_units,activation=\"relu\")\n",
        "    self.dropout=layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "    if nb_classes==2:\n",
        "      self.last_dense=layers.Dense(units=1,activation=\"sigmoid\")\n",
        "    else:\n",
        "      self.dense_last=layers.Dense(units=nb.nb_classes,activation=\"softmax\")\n",
        "\n",
        "def call(self,input,train):\n",
        "  self.embedding(input)\n",
        "  x1=self.bigram(x)\n",
        "  x1=self.pool1(x1)\n",
        "  x2=self.trigram(x)\n",
        "  x2=self.pool1(x2)\n",
        "  x3=self.fourgram(x)\n",
        "  x3=self.pool1(x3)\n",
        "  merged=tf.conact(x_1,x_2,x_3,axis=1)\n",
        "  merged=self.dense_1(merged)\n",
        "  merged=self.dropout(merged,train)\n",
        "  output=self.last_dense(merged)\n",
        "\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z55oHvI1N-Z5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CONFIGURATION\n",
        "\n",
        "Vocab_size=tokenizer.vocab_size\n",
        "Emb_dim=200\n",
        "NB_filters=100\n",
        "FFN_units=256\n",
        "NB_classes=len(set(train))\n",
        "Dropout_rate=0.2\n",
        "Batch_size=32\n",
        "NP_epochs=5\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jll0-2dXQEYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TRAINING\n",
        "\n",
        "Dcnn=DCNN(vocab_size=Vocab_size,\n",
        "emb_dim=Emb_dim\n",
        "nb_filters=NB_filters\n",
        "ffn_units=FFN_units\n",
        "nb_classes=NB_classes\n",
        "Dropout_rate=Dropout_rate\n",
        ")\n",
        "\n",
        "if NB_classes==2:\n",
        "  Dcnn.compile(optimizer=\"adam\",\n",
        "               loss=\"binary_crossentropy\",\n",
        "               metrics=\"accuracy\")\n",
        "\n",
        "else:\n",
        "  Dcnn.compile(optimizer=\"adam\",\n",
        "               loss=\"sparse_categorical_crossentropy\",\n",
        "               metrics=\"sparse_categorical_accuracy\")\n",
        "  \n",
        "\n",
        "Dcnn.fit(train,batch_size=Batch_size),np_epochs=Np_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tfa0fDuGR4a-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results=Dcnn.evaluate(test,test_labes,batch_size=Batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}